{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927cda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347104e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import core.dataset as dataset\n",
    "import core.get_features as get_feats\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e0a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/rationales/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "tensor([[  5,   8, 138],\n",
      "        [ 16,   7, 271],\n",
      "        [ 10,   7, 375],\n",
      "        [ 23,   7,  62]])\n"
     ]
    }
   ],
   "source": [
    "import experiment_consistency as experiment\n",
    "experiment.analyse_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_datasets = [\n",
    "    dataset.CriterionDataset(\"data/lines/criterion\", \"img_annotations.csv\",\n",
    "                         \"class_names.csv\", only_class_id, dataset.preprocess)\n",
    "    for only_class_id in [0, 1, 2, 3]\n",
    "]\n",
    "\n",
    "model = vgg16(pretrained = True)\n",
    "\n",
    "feature_map_log = []\n",
    "hooked_layers = []\n",
    "feature_map_recorder = get_feats.get_feature_map_recorder(feature_map_log)\n",
    "get_feats.register_hooks(model, hooked_layers, feature_map_recorder, feature_map_log)\n",
    "\n",
    "batch_size = 1\n",
    "class_dataloaders = [torch.utils.data.DataLoader(class_dataset, batch_size=batch_size,\n",
    "                                                 shuffle=False, num_workers=2)\n",
    "                     for class_dataset in class_datasets]\n",
    "\n",
    "get_feats.forward_pass(model, class_dataloaders, feature_map_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf85319",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmaps = feature_map_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad12bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.get_features as get_feats\n",
    "fmaps = get_feats.load_feature_maps(\"./data/lines\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8569d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "50\n",
      "64\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(len(fmaps[0]))\n",
    "print(len(fmaps[0][0]))\n",
    "print(len(fmaps[0][0][0]))\n",
    "print(fmaps[0][0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aedea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3827, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.7611, 0.7980, 0.1716, 0.1346,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 2.2473, 2.3614, 0.0000, 0.0000, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.9240, 0.9447, 0.0000, 0.0000, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949, 0.4949,\n",
      "        0.4949, 0.4949, 0.4949, 0.4884])\n"
     ]
    }
   ],
   "source": [
    "print(fmaps[0][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba919943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats, class_fitnesses, most_fit_for_class = get_feats.analyse_log(\n",
    "    fmaps, get_feats.FitnessFunc.mean_deviation_fitness, save_dir = \"data/lines\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d79700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "4\n",
      "64\n",
      "tensor(0.5268)\n"
     ]
    }
   ],
   "source": [
    "print(len(class_stats[\"mean\"]))\n",
    "print(len(class_stats[\"mean\"][0]))\n",
    "print(len(class_stats[\"mean\"][0][0]))\n",
    "print(class_stats[\"mean\"][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d6ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7196, 0.9753, 1.6032, 0.7277, 1.6579, 1.2911, 0.5065, 1.2994, 0.2391,\n",
      "        0.8003, 0.9519, 1.2000, 1.7606, 1.3591, 1.5642, 0.6404, 1.2404, 1.7285,\n",
      "        0.7940, 2.4338, 0.2416, 1.2955, 1.0472, 0.7531, 0.2507, 0.9786, 0.8659,\n",
      "        1.1012, 0.7237, 0.4785, 1.7533, 2.8634, 1.0003, 0.9559, 0.5785, 2.5043,\n",
      "        1.2019, 0.7921, 0.2308, 0.6877, 1.0364, 1.6244, 0.8441, 1.3441, 0.3865,\n",
      "        0.3670, 0.4028, 0.7869, 1.1391, 1.2504, 1.2715, 1.0962, 1.2367, 2.0976,\n",
      "        0.7135, 0.9968, 1.1581, 1.7416, 1.1810, 1.6788, 0.6653, 0.3984, 0.2529,\n",
      "        0.7947, 0.8634, 0.8539, 1.1001, 0.5758, 1.0354, 1.5092, 1.5941, 1.2536,\n",
      "        0.4910, 0.7334, 0.9689, 0.6899, 2.2387, 1.7336, 1.3647, 0.6438, 1.2661,\n",
      "        0.5339, 1.0537, 0.9707, 1.0259, 0.8917, 1.8610, 1.1544, 0.2744, 0.2786,\n",
      "        0.8538, 1.6160, 0.8109, 1.2804, 1.1091, 1.2592, 1.0910, 0.7366, 0.6255,\n",
      "        1.3204, 0.7013, 0.7006, 0.2621, 1.2725, 1.1784, 1.6571, 1.2978, 1.8988,\n",
      "        1.5429, 0.6964, 0.6808, 1.1221, 0.6174, 1.1575, 2.8967, 0.5689, 1.8240,\n",
      "        3.3393, 0.9676, 1.0768, 0.5781, 0.7980, 1.2866, 0.7478, 1.2091, 1.1127,\n",
      "        1.0840, 1.2974])\n"
     ]
    }
   ],
   "source": [
    "print(class_stats[\"std\"][2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d6d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "64\n",
      "tensor(0.0031)\n"
     ]
    }
   ],
   "source": [
    "print(len(class_fitnesses))\n",
    "print(len(class_fitnesses[0]))\n",
    "print(len(class_fitnesses[0][0]))\n",
    "print(class_fitnesses[3][3][63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = experiment.assemble_and_validate_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80688056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import make_dot\n",
    "g = make_dot(network, network.state_dict())\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from torchvision.models import vgg16\n",
    "from core.generate_data import make_line_imgs\n",
    "import core.dataset as dataset\n",
    "import core.get_features as get_feats\n",
    "import core.get_rationales as get_rats\n",
    "import core.validate as val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56196d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_line_imgs([0, 45, 90, 135], 60, 256, \"random\", \"data/lines/criterion\")\n",
    "\n",
    "class_datasets = [\n",
    "    dataset.CriterionDataset(\"data/lines/criterion\", \"img_annotations.csv\",\n",
    "                         \"class_names.csv\", only_class_id, dataset.preprocess)\n",
    "    for only_class_id in [0, 1, 2, 3]\n",
    "]\n",
    "\n",
    "model = vgg16(pretrained = True)\n",
    "\n",
    "feature_map_log = []\n",
    "hooked_layers = []\n",
    "feature_map_recorder = get_feats.get_feature_map_recorder(feature_map_log)\n",
    "get_feats.register_hooks(model, hooked_layers, feature_map_recorder, feature_map_log)\n",
    "\n",
    "batch_size = 1\n",
    "class_dataloaders = [torch.utils.data.DataLoader(class_dataset, batch_size=batch_size,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "                     for class_dataset in class_datasets]\n",
    "\n",
    "get_feats.forward_pass(model, class_dataloaders, feature_map_log)\n",
    "\n",
    "get_feats.save_feature_maps(\"data/lines\", feature_map_log, hooked_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_log, _ = get_feats.load_feature_maps(\"data/lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a16991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats, class_fitnesses, most_fit_for_class = get_feats.analyse_log(\n",
    "    feature_map_log, save_dir = \"data/lines\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c36ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode = False)\n",
    "kurtosis = class_stats[\"kurtosis\"][0][0]\n",
    "skew = class_stats[\"skew\"][0][0]\n",
    "std = class_stats[\"std\"][0][0]\n",
    "mean = class_stats[\"mean\"][0][0]\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "print(kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.get_rationales as get_rats\n",
    "import core.get_features as get_feats\n",
    "\n",
    "class_stats, class_fitnesses, class_feature_map_idxs = get_feats.load_log(\"data/lines\")\n",
    "\n",
    "print(class_feature_map_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31967b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(len(class_expectations[0][0]))\n",
    "num_classes = len(class_feature_map_idxs)\n",
    "#rationale_output_expectations[i][j] := expectation of jth class's feature map for the ith class\n",
    "rationale_output_expectations = torch.zeros(num_classes, num_classes)\n",
    "for feature_map_class_idx, (fitness, conv_idx, feature_map_idx) in enumerate(class_feature_map_idxs):\n",
    "    for class_idx in range(num_classes):\n",
    "        rationale_output_expectations[class_idx][feature_map_class_idx] = class_expectations[conv_idx][class_idx][feature_map_idx]\n",
    "print(rationale_output_expectations)\n",
    "print(rationale_output_expectations.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import torch\n",
    "model = vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab31725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.dataset\n",
    "class_datasets = [\n",
    "    core.dataset.Dataset(\"data/lines\", \"img_annotations.csv\",\n",
    "                         \"class_names.csv\", only_class_id, core.dataset.preprocess)\n",
    "    for only_class_id in [0, 1, 2, 3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "im = Image.open(\"data/lines/0/0_2.png\")\n",
    "#im.show()\n",
    "t_im = transforms.ToTensor()(im)\n",
    "print(t_im.shape)\n",
    "class_idx, class_img_idx = 0, 0\n",
    "img, class_idx = class_datasets[class_idx][class_img_idx]\n",
    "print(img.unsqueeze(0).shape)\n",
    "t_img = transforms.ToPILImage()(img)\n",
    "t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede855c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deepest_class_feature_map_conv_idx = torch.max(class_feature_map_idxs[:,1])\n",
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "logits = classifier(img.unsqueeze(0))\n",
    "\n",
    "print(\"logits\", logits)\n",
    "print(\"trues\", rationale_output_expectations.diagonal())\n",
    "\n",
    "loss = logits - rationale_output_expectations.diagonal()\n",
    "mseloss = torch.nn.functional.mse_loss(logits, rationale_output_expectations.diagonal().unsqueeze(0), reduction = \"none\")\n",
    "sorted_mse_loss = torch.sort(mseloss, 1).values\n",
    "top_2_mse_loss = sorted_mse_loss[:, :2][0]\n",
    "top_2_mse_weight = top_2_mse_loss[1] - top_2_mse_loss[0]\n",
    "mean_mse_weight = torch.mean(sorted_mse_loss[:, 1:][0]) - sorted_mse_loss[:, 0][0]\n",
    "\n",
    "torch.set_printoptions(sci_mode = False)\n",
    "print(\"loss\", loss)\n",
    "print(\"mseloss\", mseloss)\n",
    "sorted_logit_sizes = torch.sort(logits, 1).values\n",
    "top_2_logit_sizes = sorted_logit_sizes[:, -2:][0]\n",
    "top_2_max_weight = top_2_logit_sizes[-1] - top_2_logit_sizes[-2]\n",
    "mean_max_weight = sorted_logit_sizes[:, -1][0] - torch.mean(sorted_logit_sizes[:, :-1][0])\n",
    "selected_classes = torch.min(mseloss, dim = 1).indices\n",
    "max_selected_classes = torch.max(logits, dim = 1).indices\n",
    "\n",
    "logit_mean_diffs = []\n",
    "logit_max_diffs = []\n",
    "for i in range(len(logits[0])):\n",
    "    other_class_idxs = [j for j in range(len(logits[0]))]\n",
    "    other_class_logits = logits[:, other_class_idxs]\n",
    "    max_other = torch.max(other_class_logits)\n",
    "    diff__ = logits[0][i] - max_other\n",
    "    logit_max_diffs.append(diff__)\n",
    "    \n",
    "    logit_i_diffs = []\n",
    "    for j in range(len(logits[0])):\n",
    "        if i != j:\n",
    "            logit_i_diffs.append((logits[0][i] - logits[0][j]))\n",
    "    logit_mean_diffs.append(torch.mean(torch.tensor(logit_i_diffs)))\n",
    "    \n",
    "logit_mean_diffs = torch.tensor(logit_mean_diffs).unsqueeze(0)\n",
    "sorted_logit_mean_diffs = torch.sort(logit_mean_diffs, 1).values\n",
    "top_2_logit_mean_diffs = sorted_logit_mean_diffs[:, -2:][0]\n",
    "top_2_logit_mean_diff_weight = top_2_logit_mean_diffs[1] - top_2_logit_mean_diffs[0]\n",
    "mean_logit_mean_diff_weight = sorted_logit_mean_diffs[:, -1][0] - torch.mean(sorted_logit_mean_diffs[:, :-1][0])\n",
    "print(\"logit mean diffs\", logit_mean_diffs)\n",
    "mean_diff_selected_classes = torch.max(logit_mean_diffs, dim = 1).indices\n",
    "\n",
    "logit_max_diffs = torch.tensor(logit_max_diffs).unsqueeze(0)\n",
    "sorted_logit_max_diffs = torch.sort(logit_max_diffs, 1).values\n",
    "top_2_logit_max_diffs = sorted_logit_max_diffs[:, -2:][0]\n",
    "top_2_logit_max_diff_weight = top_2_logit_max_diffs[1] - top_2_logit_max_diffs[0]\n",
    "mean_logit_max_diff_weight = sorted_logit_max_diffs[:, -1][0] - torch.mean(sorted_logit_max_diffs[:, :-1][0])\n",
    "print(\"logit max diffs\", logit_max_diffs)\n",
    "max_diff_selected_classes = torch.max(logit_max_diffs, dim = 1).indices\n",
    "\n",
    "poss_selections = [selected_classes, max_selected_classes, mean_diff_selected_classes]\n",
    "top_2_weights = torch.tensor([top_2_mse_weight, top_2_max_weight, top_2_logit_mean_diff_weight])\n",
    "mean_weights = torch.tensor([mean_mse_weight, mean_max_weight, mean_logit_mean_diff_weight])\n",
    "biggest_top_2_weight_idx = torch.argmax(top_2_weights)\n",
    "biggest_mean_weight_idx = torch.argmax(mean_weights)\n",
    "top_2_weighted_selection = poss_selections[biggest_top_2_weight_idx]\n",
    "mean_weighted_selection = poss_selections[biggest_mean_weight_idx]\n",
    "\n",
    "print(\"top 2 weights\", top_2_weights)\n",
    "print(\"mean weights\", mean_weights)\n",
    "print(\"diff selected\", selected_classes)\n",
    "print(\"max selected\", max_selected_classes)\n",
    "print(\"mean diff selected\", mean_diff_selected_classes)\n",
    "print(\"max diff selected\", max_diff_selected_classes)\n",
    "print(\"top 2 weighted selection\", top_2_weighted_selection)\n",
    "print(\"mean weighted selection\", mean_weighted_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataset import ValidationDataset, preprocess\n",
    "ds = ValidationDataset(\"data/lines/validation\", preprocess)\n",
    "n_samples = 1000\n",
    "dl = [ds[idx] for idx in torch.randint(len(ds), (n_samples,))]\n",
    "dl = [(d[0].unsqueeze(0), d[1].unsqueeze(0)) for d in dl]\n",
    "\n",
    "import core.validate as val\n",
    "deepest_class_feature_map_conv_idx = torch.max(class_feature_map_idxs[:,1])\n",
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "metrics = [val.get_ideal_vs_observed_class_expectations, val.get_max_expectation, val.get_most_extreme_observation]\n",
    "val.validate(classifier, dl, class_expectations, class_feature_map_idxs, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1011fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "classifier(torch.randn(1,3,256,256))\n",
    "classifier(torch.randn(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326daf99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from core.dataset import ValidationDataset, preprocess\n",
    "ds = ValidationDataset(\"data/lines/validation\", preprocess)\n",
    "batch_size = 1\n",
    "img = ds.get_class_item(0, 3)\n",
    "t_img = transforms.ToPILImage()(img)\n",
    "t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1a45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
