{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aec034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d42c3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   9, 375],\n",
      "        [  4,   7, 322],\n",
      "        [  3,   7, 375],\n",
      "        [  4,   7,  62]])\n"
     ]
    }
   ],
   "source": [
    "import core.get_rationales as get_rats\n",
    "import core.get_features as get_feats\n",
    "\n",
    "class_expectations, class_fitnesses, class_feature_map_idxs = get_feats.load_log(\"data/lines\")\n",
    "\n",
    "print(class_feature_map_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31967b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tensor([[2.3315, 0.4176, 1.8571, 1.6193],\n",
      "        [0.3206, 4.5085, 1.4950, 0.9994],\n",
      "        [0.1533, 0.2692, 5.1348, 1.3193],\n",
      "        [0.5211, 0.2942, 1.7720, 6.0899]])\n",
      "tensor([2.3315, 4.5085, 5.1348, 6.0899])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(len(class_expectations[0][0]))\n",
    "num_classes = len(class_feature_map_idxs)\n",
    "#rationale_output_expectations[i][j] := expectation of jth class's feature map for the ith class\n",
    "rationale_output_expectations = torch.zeros(num_classes, num_classes)\n",
    "for feature_map_class_idx, (fitness, conv_idx, feature_map_idx) in enumerate(class_feature_map_idxs):\n",
    "    for class_idx in range(num_classes):\n",
    "        rationale_output_expectations[class_idx][feature_map_class_idx] = class_expectations[conv_idx][class_idx][feature_map_idx]\n",
    "print(rationale_output_expectations)\n",
    "print(rationale_output_expectations.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import torch\n",
    "model = vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab31725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.dataset\n",
    "class_datasets = [\n",
    "    core.dataset.Dataset(\"data/lines\", \"img_annotations.csv\",\n",
    "                         \"class_names.csv\", only_class_id, core.dataset.preprocess)\n",
    "    for only_class_id in [0, 1, 2, 3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "im = Image.open(\"data/lines/0/0_2.png\")\n",
    "#im.show()\n",
    "t_im = transforms.ToTensor()(im)\n",
    "print(t_im.shape)\n",
    "class_idx, class_img_idx = 0, 0\n",
    "img, class_idx = class_datasets[class_idx][class_img_idx]\n",
    "print(img.unsqueeze(0).shape)\n",
    "t_img = transforms.ToPILImage()(img)\n",
    "t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede855c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deepest_class_feature_map_conv_idx = torch.max(class_feature_map_idxs[:,1])\n",
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "logits = classifier(img.unsqueeze(0))\n",
    "\n",
    "print(\"logits\", logits)\n",
    "print(\"trues\", rationale_output_expectations.diagonal())\n",
    "\n",
    "loss = logits - rationale_output_expectations.diagonal()\n",
    "mseloss = torch.nn.functional.mse_loss(logits, rationale_output_expectations.diagonal().unsqueeze(0), reduction = \"none\")\n",
    "sorted_mse_loss = torch.sort(mseloss, 1).values\n",
    "top_2_mse_loss = sorted_mse_loss[:, :2][0]\n",
    "top_2_mse_weight = top_2_mse_loss[1] - top_2_mse_loss[0]\n",
    "mean_mse_weight = torch.mean(sorted_mse_loss[:, 1:][0]) - sorted_mse_loss[:, 0][0]\n",
    "\n",
    "torch.set_printoptions(sci_mode = False)\n",
    "print(\"loss\", loss)\n",
    "print(\"mseloss\", mseloss)\n",
    "sorted_logit_sizes = torch.sort(logits, 1).values\n",
    "top_2_logit_sizes = sorted_logit_sizes[:, -2:][0]\n",
    "top_2_max_weight = top_2_logit_sizes[-1] - top_2_logit_sizes[-2]\n",
    "mean_max_weight = sorted_logit_sizes[:, -1][0] - torch.mean(sorted_logit_sizes[:, :-1][0])\n",
    "selected_classes = torch.min(mseloss, dim = 1).indices\n",
    "max_selected_classes = torch.max(logits, dim = 1).indices\n",
    "\n",
    "logit_mean_diffs = []\n",
    "logit_max_diffs = []\n",
    "for i in range(len(logits[0])):\n",
    "    other_class_idxs = [j for j in range(len(logits[0]))]\n",
    "    other_class_logits = logits[:, other_class_idxs]\n",
    "    max_other = torch.max(other_class_logits)\n",
    "    diff__ = logits[0][i] - max_other\n",
    "    logit_max_diffs.append(diff__)\n",
    "    \n",
    "    logit_i_diffs = []\n",
    "    for j in range(len(logits[0])):\n",
    "        if i != j:\n",
    "            logit_i_diffs.append((logits[0][i] - logits[0][j]))\n",
    "    logit_mean_diffs.append(torch.mean(torch.tensor(logit_i_diffs)))\n",
    "    \n",
    "logit_mean_diffs = torch.tensor(logit_mean_diffs).unsqueeze(0)\n",
    "sorted_logit_mean_diffs = torch.sort(logit_mean_diffs, 1).values\n",
    "top_2_logit_mean_diffs = sorted_logit_mean_diffs[:, -2:][0]\n",
    "top_2_logit_mean_diff_weight = top_2_logit_mean_diffs[1] - top_2_logit_mean_diffs[0]\n",
    "mean_logit_mean_diff_weight = sorted_logit_mean_diffs[:, -1][0] - torch.mean(sorted_logit_mean_diffs[:, :-1][0])\n",
    "print(\"logit mean diffs\", logit_mean_diffs)\n",
    "mean_diff_selected_classes = torch.max(logit_mean_diffs, dim = 1).indices\n",
    "\n",
    "logit_max_diffs = torch.tensor(logit_max_diffs).unsqueeze(0)\n",
    "sorted_logit_max_diffs = torch.sort(logit_max_diffs, 1).values\n",
    "top_2_logit_max_diffs = sorted_logit_max_diffs[:, -2:][0]\n",
    "top_2_logit_max_diff_weight = top_2_logit_max_diffs[1] - top_2_logit_max_diffs[0]\n",
    "mean_logit_max_diff_weight = sorted_logit_max_diffs[:, -1][0] - torch.mean(sorted_logit_max_diffs[:, :-1][0])\n",
    "print(\"logit max diffs\", logit_max_diffs)\n",
    "max_diff_selected_classes = torch.max(logit_max_diffs, dim = 1).indices\n",
    "\n",
    "poss_selections = [selected_classes, max_selected_classes, mean_diff_selected_classes]\n",
    "top_2_weights = torch.tensor([top_2_mse_weight, top_2_max_weight, top_2_logit_mean_diff_weight])\n",
    "mean_weights = torch.tensor([mean_mse_weight, mean_max_weight, mean_logit_mean_diff_weight])\n",
    "biggest_top_2_weight_idx = torch.argmax(top_2_weights)\n",
    "biggest_mean_weight_idx = torch.argmax(mean_weights)\n",
    "top_2_weighted_selection = poss_selections[biggest_top_2_weight_idx]\n",
    "mean_weighted_selection = poss_selections[biggest_mean_weight_idx]\n",
    "\n",
    "print(\"top 2 weights\", top_2_weights)\n",
    "print(\"mean weights\", mean_weights)\n",
    "print(\"diff selected\", selected_classes)\n",
    "print(\"max selected\", max_selected_classes)\n",
    "print(\"mean diff selected\", mean_diff_selected_classes)\n",
    "print(\"max diff selected\", max_diff_selected_classes)\n",
    "print(\"top 2 weighted selection\", top_2_weighted_selection)\n",
    "print(\"mean weighted selection\", mean_weighted_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ca6dfcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6a9eadb95816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValidationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/lines/validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-6a9eadb95816>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValidationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/lines/validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/andre/Documents/programming/rationales/core/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/andre/Documents/programming/rationales/core/dataset.py\u001b[0m in \u001b[0;36mget_class_item\u001b[0;34m(self, img_idx, class_idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         class_transform = transforms.Compose([\n",
      "\u001b[0;32m~/miniconda3/envs/rationales/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rationales/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rationales/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from core.dataset import ValidationDataset, preprocess\n",
    "ds = ValidationDataset(\"data/lines/validation\", preprocess)\n",
    "n_samples = 1000\n",
    "dl = [ds[idx] for idx in torch.randint(len(ds), (n_samples,))]\n",
    "dl = [(d[0].unsqueeze(0), d[1].unsqueeze(0)) for d in dl]\n",
    "\n",
    "import core.validate as val\n",
    "deepest_class_feature_map_conv_idx = torch.max(class_feature_map_idxs[:,1])\n",
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "metrics = [val.get_ideal_vs_observed_class_expectations, val.get_max_expectation, val.get_most_extreme_observation]\n",
    "val.validate(classifier, dl, class_expectations, class_feature_map_idxs, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb1011fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0257,  0.1595, 11.0526,  1.1198]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_class_rationale = get_rats.get_rationale(model, deepest_class_feature_map_conv_idx)\n",
    "ClassifierNetwork = get_rats.rationale_to_classifier_network(multi_class_rationale, class_feature_map_idxs)\n",
    "classifier = ClassifierNetwork()\n",
    "classifier(torch.randn(1,3,256,256))\n",
    "classifier(torch.randn(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326daf99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from core.dataset import ValidationDataset, preprocess\n",
    "ds = ValidationDataset(\"data/lines/validation\", preprocess)\n",
    "batch_size = 1\n",
    "img = ds.get_class_item(0, 3)\n",
    "t_img = transforms.ToPILImage()(img)\n",
    "t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1a45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
