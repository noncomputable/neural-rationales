{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52ec145",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcd7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77c46e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_coords(n_lines, angle):   \n",
    "    angle = angle * math.pi / 180\n",
    "    coords = np.random.randint(64, size = (n_lines, 4))\n",
    "    for coord in coords:\n",
    "        x1, y1 = coord[0], coord[1]\n",
    "        x2_dist, y2_dist = np.random.randint(x1, 64), np.random.randint(y1, 64)\n",
    "        direction = 1 if np.random.random() < .5 else -1\n",
    "        coord[2] = coord[0] + direction * math.sin(angle) * x2_dist\n",
    "        coord[3] = coord[1] + direction * math.cos(angle) * y2_dist\n",
    "    return coords\n",
    "\n",
    "def draw_lines(img_draw, angle):\n",
    "    n_lines = np.random.randint(3, 10)\n",
    "    line_coords = get_line_coords(n_lines, angle)\n",
    "    colors = np.random.randint(256, size = (n_lines, 3))\n",
    "    for (x1, y1, x2, y2), (r, g, b) in zip(line_coords, colors):\n",
    "        img_draw.line((x1, y1, x2, y2), fill=(r,g,b), width=2)\n",
    "\n",
    "def get_line_img(angle, bg_color):\n",
    "    img = Image.new(\"RGB\", (64, 64), bg_color)\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    draw_lines(img_draw, angle)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def save_line_imgs(angles, num_per_angle, path):\n",
    "    class_annotations = {\"class_name\": angles}\n",
    "    img_annotations = {\"img_class\": [], \"class_img_idx\": []}\n",
    "    \n",
    "    n_images = len(angles) * num_per_angle\n",
    "    bg_colors = np.full((n_images, 3), 255) #np.random.randint(256, size = (n_images, 3))\n",
    "    for i, angle in enumerate(angles):\n",
    "        os.mkdir(f\"{path}/{angle}\")\n",
    "        for j in range(num_per_angle):\n",
    "            img_annotations[\"img_class\"].append(i)\n",
    "            img_annotations[\"class_img_idx\"].append(j)\n",
    "            bg_color = tuple(bg_colors[num_per_angle*i + j])\n",
    "            img = get_line_img(angle, bg_color)\n",
    "            img.save(f\"{path}/{angle}/{angle}_{j}.jpg\")\n",
    "            \n",
    "    pd.DataFrame(class_annotations).to_csv(f\"{path}/class_names.csv\", index = False, header = True)\n",
    "    pd.DataFrame(img_annotations).to_csv(f\"{path}/img_annotations.csv\", index = False, header = True)\n",
    "\n",
    "save_line_imgs([0, 45, 90, 135], 50, \"data/lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523cc01",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337c1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, label_names_file, only_class_id = None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        only_class_id - Only load data for the class with this id.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.img_labels = pd.read_csv(f\"{img_dir}/{annotations_file}\")    \n",
    "        if only_class_id is not None:\n",
    "            self.img_labels = self.img_labels.loc[self.img_labels[\"img_class\"] == only_class_id]\n",
    "        self.only_class_id = only_class_id     \n",
    "        label_names = pd.read_csv(f\"{img_dir}/{label_names_file}\")\n",
    "        self.class_idx_to_name = list(label_names[\"class_name\"])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        class_idx, class_img_idx = self.img_labels.iloc[idx]\n",
    "        image, class_idx = self.get_class_item(class_idx, class_img_idx)\n",
    "        \n",
    "        return image, class_idx\n",
    "\n",
    "    def get_class_item(self, class_idx, class_img_idx):\n",
    "        class_name = self.class_idx_to_name[class_idx]\n",
    "        img_path = f\"{self.img_dir}/{class_name}/{class_name}_{class_img_idx}.jpg\"\n",
    "        image = read_image(img_path).float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, class_idx\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7b824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_datasets = [Dataset(\"data/lines\", \"img_annotations.csv\", \"class_names.csv\", only_class_id, preprocess)\n",
    "                  for only_class_id in [0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddc3dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaa3ff",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f672d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "model = vgg16(pretrained = True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121bd71",
   "metadata": {},
   "source": [
    "### Add hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff6039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_map[i][j][k][m] := mth feature map of the ith layer of the network for data sample k of class j\n",
    "feature_map = []\n",
    "\n",
    "def record_feature_map(module, input, output):\n",
    "    feature_map[module.id][-1].extend(list(output))\n",
    "\n",
    "def register_hooks(model, hooked_layers):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, torch.nn.Conv2d):\n",
    "            child.id = len(hooked_layers)\n",
    "            hooked_layers.append(str(child))\n",
    "            child.register_forward_hook(record_feature_map)\n",
    "            feature_map.append([])\n",
    "        elif isinstance(child, torch.nn.Module):\n",
    "            register_hooks(child, hooked_layers)\n",
    "    \n",
    "    return hooked_layers\n",
    "\n",
    "hooked_layers = register_hooks(model, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ce553",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b1a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "class_dataloaders = [torch.utils.data.DataLoader(class_dataset, batch_size=batch_size,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "                     for class_dataset in class_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Debug\n",
    "images, labels = next(iter(class_dataloaders[0]))\n",
    "out = model.forward(images)\n",
    "print(out)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de787e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/rationales/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     for i, class_dataloader in enumerate(class_dataloaders):\n",
    "        #Add a list for feature maps for this class at each layer.\n",
    "        for layer in feature_map:\n",
    "             layer.append([])\n",
    "                \n",
    "        for j, batch in enumerate(class_dataloader):\n",
    "            if j % 10 == 0: print(j)\n",
    "            images, class_idxs = batch\n",
    "            logits = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ff2209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "4\n",
      "50\n",
      "torch.Size([64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(len(hooked_layers))\n",
    "print(len(feature_map))\n",
    "print(len(feature_map[0]))\n",
    "print(len(feature_map[0][0]))\n",
    "print(feature_map[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f9f10",
   "metadata": {},
   "source": [
    "### Save feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba62e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc40953",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(feature_map, \"data/lines/feature_maps.pt\")\n",
    "\n",
    "with open(\"data/lines/layer_list.json\", \"w\") as layer_list:\n",
    "    json.dump(hooked_layers, layer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978ddcb",
   "metadata": {},
   "source": [
    "### Load feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = torch.load(\"data/lines/feature_maps.pt\")\n",
    "\n",
    "with open(\"data/lines/layer_list.json\", \"r\") as layer_list:\n",
    "    layer_list = json.load(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f284942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d20497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(layer_list))\n",
    "print(len(feature_map))\n",
    "print(len(feature_map[0]))\n",
    "print(len(feature_map[0][0]))\n",
    "print(feature_map[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0445fd",
   "metadata": {},
   "source": [
    "### Get expected value of each feature map for each class\n",
    "Each feature map can have many features generated by applying one kernel (one set of weights) to the output of the prev layer. So we compare classes by taking the average of the features in each feature map for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ca76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feature_map_class_expectations[i][j][m] := ave values of the mth feature map\n",
    "                                              of the ith layer\n",
    "                                              in the jth class\n",
    "\"\"\"\n",
    "feature_map_class_expectations = []\n",
    "\n",
    "for i, layer_feature_maps in enumerate(feature_map):\n",
    "    layer = feature_map[i]\n",
    "    feature_map_class_expectations.append([])\n",
    "    for j, layer_feature_maps_for_class in enumerate(layer_feature_maps):\n",
    "        layer_feature_maps_for_class = torch.stack(layer_feature_maps_for_class)\n",
    "        #Reduce over samples and features, only distinguish by feature maps.\n",
    "        feature_map_expectations_for_class = torch.mean(layer_feature_maps_for_class, dim = [0,2,3])\n",
    "        feature_map_class_expectations[-1].append(feature_map_expectations_for_class)\n",
    "    feature_map_class_expectations[-1] = torch.stack(feature_map_class_expectations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102c422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([4, 64])\n",
      "torch.Size([64])\n",
      "tensor(50.4084)\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_map_class_expectations))\n",
    "print(feature_map_class_expectations[0].shape)\n",
    "print(feature_map_class_expectations[0][0].shape)\n",
    "print(feature_map_class_expectations[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a123d",
   "metadata": {},
   "source": [
    "### Find the feature map with the highest fitness for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40759b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A feature map's fitness for a class will be its expectation minus the max expectation of the *other* classes.\n",
    "\n",
    "feature_map_class_fitness[i][j][m] := fitness of the mth feature map\n",
    "                                      of the ith layer\n",
    "                                      for the jth class\n",
    "\"\"\"\n",
    "feature_map_class_fitnesses = []\n",
    "\n",
    "#most_fit_feature_map_for_class[j] := (fitness, layer index, feature map index)\n",
    "most_fit_feature_map_for_class = torch.full((len(feature_map_class_expectations[0]), 3), -1)\n",
    "\n",
    "for i, layer_feature_map_class_expectations in enumerate(feature_map_class_expectations):\n",
    "    layer_all_class_fitnesses = []\n",
    "    for class_j, (fitness, _, _) in enumerate(most_fit_feature_map_for_class):\n",
    "        other_class_idxs = [i for i in range(len(most_fit_feature_map_for_class)) if i != class_j]\n",
    "        max_expectations_for_other_classes = torch.max(\n",
    "            layer_feature_map_class_expectations[other_class_idxs], dim = 0\n",
    "        ).values\n",
    "        layer_class_fitnesses = layer_feature_map_class_expectations[class_j] - max_expectations_for_other_classes\n",
    "        layer_all_class_fitnesses.append(layer_class_fitnesses)\n",
    "        most_fit_feature_map_in_layer_for_class = torch.max(layer_class_fitnesses, dim = 0)\n",
    "        if most_fit_feature_map_in_layer_for_class.values > fitness:\n",
    "            most_fit_feature_map_for_class[class_j, 0] = most_fit_feature_map_in_layer_for_class.values\n",
    "            most_fit_feature_map_for_class[class_j, 1] = i\n",
    "            most_fit_feature_map_for_class[class_j, 2] = most_fit_feature_map_in_layer_for_class.indices\n",
    "    feature_map_class_fitnesses.append(torch.stack(layer_all_class_fitnesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef490f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[853,   5, 158],\n",
       "        [690,   5, 210],\n",
       "        [924,   4, 139],\n",
       "        [700,   7,  62]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_fit_feature_map_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7246e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5478.9307)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(feature_map[5][3][45][158])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ade91d",
   "metadata": {},
   "source": [
    "### Notebook Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0daa8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "global_mem_usage = sorted([\n",
    "        (x, sys.getsizeof(globals().get(x)))\n",
    "        for x in dir()\n",
    "        if not x.startswith('_') and x not in sys.modules and x not in ipython_vars\n",
    "       ], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "import inspect\n",
    "def get_ob_mem_usage(ob):\n",
    "    ob_mem_usage = sorted([\n",
    "                    (x[0], sys.getsizeof(getattr(ob,x[0])))\n",
    "                    for x in inspect.getmembers(ob, lambda a:not(inspect.isroutine(a)))\n",
    "                    if not x[0].startswith('_')\n",
    "                   ], key=lambda y: y[1], reverse=True)\n",
    "    \n",
    "    return ob_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mem_usage = get_ob_mem_usage(class_datasets[0])\n",
    "print(dataset_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_datasets[0].img_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
